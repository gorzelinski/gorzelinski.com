---
title: Ssszukaj książek jak programista
description: 'Lubię czytać książki i spędzam sporo czasu na ich poszukiwaniu. Dlatego zautomatyzowałem ten proces ze skryptem w języku Python do web scrapingu.'
date: 2022-08-22T20:00:00+02:00
updated: 2022-08-22T20:00:00+02:00
image:
  alt: 'Ręka z lupą sprawdzająca klawiaturę laptopa'
  caption: 'Zdjęcie autorstwa Agence Olloweb'
  src: 'agence-olloweb-d9ILr-dbEdg-unsplash.jpg'
categories: ['nauka o danych', 'informatyka']
tags: ['python', 'web', 'scraping', 'książki']
type: 'post'
---

Lubię czytać książki. Różne gatunki wypełniają mój regał: ekonomia, psychologia, filozofia, biografie, literatura faktu, a w szczególności książki naukowe. Bonobo z okładki książki Fransa de Walla spogląda na wybitną twarz Einsteina. Zanim jednak cokolwiek tam postawię, dużo szukam. Potrafię spędzić dosłownie godzinę, zanim kliknę “kup” w księgarni internetowej. Poleceń szukam wszędzie: pytając znajomych, nieznajomych, przeglądając forum Quora, Medium i oczywiście - Lubimyczytać. Przeszukiwanie sieci jest fajne, ale każda godzina spędzona na szukaniu, zabiera czas od właściwego czytania. Dlatego, jak prawdziwy programista, spędźmy kolejne godziny automatyzując ten proces!

Zanim napiszę pierwszą linię kodu, muszę zaznaczyć - nie jestem programistą Pythona. Skończyłem jakieś kursy o języku Python, data science i przeczytałem książkę, ale to wszystko. Ten post jest dla mnie okazją do nauki. Możesz uczyć się ze mną. Trochę wiedzy o sieci i języku Python jest wymagane, aby kontynuować.

## Plan

Zacznijmy od planu. Co chcemy osiągnąć? Chcemy listę najlepszych książek na konkretny temat lub z konkretnej kategorii. Lubimyczytać ma ogromny katalog książek, dlatego użyjemy go. Wybieranie “najlepszych” książek będzie intersubiektywne - wykorzystamy istniejące oceny użytkowników. To nie jest tak, że znalezione książki będą najlepsze dla każdego. Ale istnieje spora szansa, że nam się spodobają. Zawierzymy mądrości tłumu. Pomyślmy o krokach, które musimy podjąć.

1. Odwiedzić stronę Lubimyczytać.
2. Zażądać listy książek z konkretnej kategorii.
3. Iterować po wielu stronach.
4. Przetworzyć odpowiedź dla każdego adresu URL.
5. Znaleźć elementy DOM ze średnią ocen i liczbą ocen.
6. Stworzyć listę z najlepszymi książkami.
7. Ustalić subiektywne zasady dodawania książek do lisy.
8. Zapisać listę do jakiegoś pliku.

## Środowisko

Będę używał Ubuntu, gdzie język Python jest preinstalowany. Jeżeli używasz innego systemu operacyjnego, tu jest [link do pobrania](https://www.python.org/downloads/). W tym projekcie będę używał Pythona 3.8.10. Będę używał także zewnętrznych modułów. Aby uniknąć problemów z wersjami, stworzymy [wirtualne środowisko](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) (dla systemu Windows, kroki mogą się trochę różnić).

```bash
python3 -m venv best-books
```

Będzie ono zawierało konkretne wersje interpretera Python i innych modułów. Po stworzeniu wirtualnego środowiska, musimy je aktywować.

```bash
source best-books/bin/activate
```

Ta komenda umieści pliki wykonywalne pip i python w ścieżce `PATH` powłoki. Aby sprawdzić czy działa, możesz wpisać:

```bash
which python
```

Powinna wyświetlić się ścieżka:

```bash
.../best-books/bin/python
```

Teraz jesteśmy gotowi, żeby napisssać pierwsze linie kodu. Dobra, nie użyję tego żartu ponownie. Zwłaszcza, że nazwa języka pochodzi od komedii “Latający Cyrk Monty Pythona”, a nie od węża.

## Zewnętrzne moduły

Przeszukałem sieć i znalazłem sporo narzędzi do web scrapingu dla języka Python. Znalazłem na przykład [Scrapy](https://scrapy.org/) - popularny, potężny i wydajny framework do budowania pająków, które będą przeszukiwały sieć. Brzmi fajnie, ale dla naszego prostego skryptu byłaby to przesada. Krzywa uczenia się jest podobno stroma. Dlatego wykorzystamy dwie prostsze biblioteki. Wykorzystamy bibliotekę `requests` do wykonywania żądań HTTP. Nazwa może nie jest ekscytująca, ale niech cię to nie zwiedzie - to jest jedna z najpopularniejszych bibliotek Pythona. Zainstalujmy ją.

```bash
pip install requests
```

Po otrzymaniu odpowiedzi, musimy wydobyć potrzebne dane ze strony internetowej. `BeautifulSoup` pomoże nam w tym. Jest to biblioteka parsująca - pomaga wyłuskać dane z plików XML i HTML. No i nazwa jest bardziej fantazyjna. Współczesne strony chyba faktycznie mogą być piękną zupą składającą się plików JavaScript, HTML i CSS.

```bash
pip install beautifulsoup4
```

## Skrypt Pythona do web scrapingu

Najpierw musimy zaimportować metodę `get` z biblioteki `requests`.

```python
from requests import get
```

Zażądajmy strony Lubimyczytać z książkami popularnonaukowymi, aby sprawdzić czy działa.

```python
from requests import get

url = "https://lubimyczytac.pl/ksiazki/k/107/literatura-popularnonaukowa"
res = get(url)

print(res.status_code)  # 200
print(res.text)  # Zawartość strony
```

```bash
python3 scrape-books.py
```

Jeżeli wyświetliłeś zawartość, zauważyłeś dużo rzeczy w terminalu. Teraz musimy zaimportować bibliotekę `BeautifulSoup`, aby to przetworzyć.

```python
from requests import get
from bs4 import BeautifulSoup

url = "https://lubimyczytac.pl/ksiazki/k/107/literatura-popularnonaukowa"
res = get(url)
soup = BeautifulSoup(res.text, "html.parser")

print(soup.prettify())  # Sformatowany HTML
```

Teraz dane wyjściowe z naszego terminala są bardziej czytelne. Książki mają klasę `authorAllBooks__single`. Możemy ją wykorzystać, aby złapać je wszystkie.

```python
from requests import get
from bs4 import BeautifulSoup

url = "https://lubimyczytac.pl/ksiazki/k/107/literatura-popularnonaukowa"
res = get(url)
soup = BeautifulSoup(res.text, "html.parser")
books = soup.select(".authorAllBooks__single")

print(books)  # Lista książek
print(len(books))  # 30
```

Musimy znaleźć elementy DOM zawierające informacje o ocenach i ich liczbie.

```html
<div class="listLibrary__rating">
  <span class="listLibrary__ratingText"> Średnia ocen: </span>
  <div class="listLibrary__ratingStars">
    <span class="listLibrary__ratingStarsImg icon-icon-star-full"></span>
    <span class="listLibrary__ratingStarsNumber"> 8,3</span> / 10
  </div>
</div>
<div class="listLibrary__ratingAll">3 ocen</div>
```

Elementy, których szukamy mają klasy `listLibrary__ratingStarsNumber` i `listLibrary__ratingAll`. Wykorzystując je możemy wydobyć oceny z książek.

```python
from requests import get
from bs4 import BeautifulSoup

url = "https://lubimyczytac.pl/ksiazki/k/107/literatura-popularnonaukowa"
res = get(url)
soup = BeautifulSoup(res.text, "html.parser")
books = soup.select(".authorAllBooks__single")

for book in books:
    average_rating = book.select_one(
        ".listLibrary__ratingStarsNumber").get_text(strip=True).replace(",", ".")
    ratings_string = book.select_one(
        ".listLibrary__ratingAll").get_text(strip=True).split()[0]
    ratings = 0 if ratings_string == "ocen" else ratings_string

    print(float(average_rating))  # Float np. 8.3
    print(int(ratings))  # Int np. 3
```

Sporo się tu dzieje, dlatego zatrzymajmy się na chwilę. Z każdej książki na stronie chcemy wydobyć informację o średniej i liczbie ocen. Na początek wybieramy `span` ze średnią ocen i wyciągamy z niego tekst. Zamieniamy przecinki na kropki, żeby móc zmienić typ danych na `float`. Następnie wybieramy element z informacją o liczbie ocen. Dzielimy ten tekst wykorzystując spacje i wybieramy z listy pierwszy element. Prawie zawsze znajduje się w nim liczba ocen, ale czasem jej brakuje. Dlatego wykorzystałem operator trójskładnikowy, żeby uwzględnić ten przypadek. Mogłem wykorzystać regex albo iterację przez tekst, aby wydobyć te informacje zamiast polegać na pozycjach, ale nie chciałem komplikować tego fragmentu. Wyciągnąłem także informacje o autorze, tytule i link.

```python
from requests import get
from bs4 import BeautifulSoup

url = "https://lubimyczytac.pl/ksiazki/k/107/literatura-popularnonaukowa"
res = get(url)
soup = BeautifulSoup(res.text, "html.parser")
books = soup.select(".authorAllBooks__single")

for book in books:
    average_rating = book.select_one(
        ".listLibrary__ratingStarsNumber").get_text(strip=True).replace(",", ".")
    ratings_string = book.select_one(
        ".listLibrary__ratingAll").get_text(strip=True).split()[0]
    ratings = 0 if ratings_string == "ocen" else ratings_string
    title = book.select_one(
        ".authorAllBooks__singleTextTitle").get_text(strip=True)
    author = book.select_one(
        ".authorAllBooks__singleTextAuthor").get_text(strip=True)
    link = book.select_one(
        ".authorAllBooks__singleTextTitle")["href"]
    print(title, author)  # Tytuł, autor dla każdej książki
```

Teraz w końcu możemy wybrać najlepsze książki. Bądźmy wybredni - książka, aby być na naszej liście musi mieć średnią powyżej 8.0 i liczbę ocen powyżej 2000. Możemy zmodyfikować zmienne: `average_rating_threshold` i `average_threshold`, aby poluzować warunki. Przed pętlą stworzyłem listę `best_books`. Informacje o książce są do niej dodane jeżeli oba warunki zostały spełnione.

```python
from requests import get
from bs4 import BeautifulSoup

base_url = "https://lubimyczytac.pl"
url = f"{base_url}/ksiazki/k/107/literatura-popularnonaukowa"
average_rating_threshold = 8.0
ratings_threshold = 2000

res = get(url)
soup = BeautifulSoup(res.text, "html.parser")
books = soup.select(".authorAllBooks__single")

best_books = []
for book in books:
    average_rating = float(book.select_one(
                    ".listLibrary__ratingStarsNumber").get_text(strip=True).replace(",", "."))
    ratings_string = book.select_one(
                    ".listLibrary__ratingAll").get_text(strip=True).split()[0]
    ratings = int(0 if ratings_string == "ocen" else ratings_string)

    if average_rating > average_rating_threshold and ratings > ratings_threshold:
        title = book.select_one(
                        ".authorAllBooks__singleTextTitle").get_text(strip=True)
        author = book.select_one(
                        ".authorAllBooks__singleTextAuthor").get_text(strip=True)
        link = book.select_one(
                        ".authorAllBooks__singleTextTitle")["href"]
        best_book = {"author": author, "title": title,
                     "average_rating": average_rating, "ratings": ratings, "link": f"{base_url}{link}"}

        best_books.append(best_book)

print(best_books)  # Lista książek ze średnią powyżej 8.0 i ponad 2000 ocen
```

Udało nam się “zeskrobać” książki z jednej strony. Zróbmy coś nawet lepszego - przeszukajmy dziesięć stron!

```python
from requests import get
from bs4 import BeautifulSoup

base_url = "https://lubimyczytac.pl"
average_rating_threshold = 8.0
rating_threshold = 2000
start_page = 1
stop_page = 11

best_books = []
for page in range(start_page, stop_page):
    url = f"{base_url}/katalog?page={page}&listId=booksFilteredList&category[]=68&rating[]=0&rating[]=10&publishedYear[]=1200&publishedYear[]=2022&catalogSortBy=ratings-desc&paginatorType=Standard"
    res = get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    books = soup.select(".authorAllBooks__single")

    for book in books:
        average_rating = float(book.select_one(
            ".listLibrary__ratingStarsNumber").get_text(strip=True).replace(",", "."))
        ratings_string = book.select_one(
            ".listLibrary__ratingAll").get_text(strip=True).split()[0]
        ratings = int(0 if ratings_string == "ocen" else ratings_string)

        if average_rating > average_rating_threshold and ratings > rating_threshold:
            title = book.select_one(
                ".authorAllBooks__singleTextTitle").get_text(strip=True)
            author = book.select_one(
                ".authorAllBooks__singleTextAuthor").get_text(strip=True)
            link = book.select_one(
                ".authorAllBooks__singleTextTitle")["href"]
            best_book = {"author": author, "title": title,
                         "average_rating": average_rating, "ratings": ratings, "link": f"{base_url}{link}"}
            best_books.append(best_book)

print(best_books)
```

Zmodyfikowałem adres URL aby zawierał aktualną stronę. Iterowałem przez strony używając funkcji `range`. Po znalezieniu książek pozostał ostatni krok - zapisanie ich do pliku. Poniżej pętli iterującej przez strony, dodałem fragment kodu, który zapisuje najlepsze książki do pliku markdown.

```python
# ...
from operator import itemgetter

# ...

subject = "popularnonaukowe"

# ...

best_books = [] # Tu śą najlepsze książki

# ...

file = open(f"najlepsze-ksiazki-{subject}.md", "w")
file.write(f"## Najlepsze książki z kategorii: {subject}\n")
for book in best_books:
    title, author, average_rating, ratings, link = itemgetter(
        "title", "author", "average_rating", "ratings", "link")(book)
    list_item = f'- [{title}]({link})<br />autorstwa {author} | <small title="Średnia ocen">{average_rating}⭐</small> <small>{ratings} ocen</small>\n'
    file.write(list_item)

file.close()
```

Wykorzystałem wbudowane funkcje języka Python: `open`, `write` i `close`, aby zapisać najlepsze książki do pliku. Pierwsza linia w pliku to nagłówek `h2` z informacją o kategorii. `itemgetter` to funkcja, która pozwala w elegancki sposób wydobyć dane ze słownika. Następnie użyłem tych danych, aby stworzyć element listy dla każdej książki. Znajduję się tam trochę znaczników HTML, aby wyglądało to ładniej. No i to wszystko - mamy najlepsze książki w pliku. Tu jest jego zawartość...

```markdown
## Najlepsze książki z kategorii: popularnonaukowe

- [Sapiens. Od zwierząt do bogów](https://lubimyczytac.pl/ksiazka/4988781/sapiens-od-zwierzat-do-bogow)<br />autorstwa Yuval Noah Harari | <small title="Średnia ocen">8.3⭐</small> <small>9123 ocen</small>
- [Sztuka obsługi penisa](https://lubimyczytac.pl/ksiazka/4818570/sztuka-obslugi-penisa)<br />autorstwa Przemysław Pilarski,Andrzej Gryżewski | <small title="Średnia ocen">8.5⭐</small> <small>3333 ocen</small>
- [Krótka historia prawie wszystkiego](https://lubimyczytac.pl/ksiazka/3749344/krotka-historia-prawie-wszystkiego)<br />autorstwa Bill Bryson | <small title="Średnia ocen">8.2⭐</small> <small>2123 ocen</small>
```

...i to jak się wyświetla:

# Najlepsze książki z kategorii: popularnonaukowe

- [Sapiens. Od zwierząt do bogów](https://lubimyczytac.pl/ksiazka/4988781/sapiens-od-zwierzat-do-bogow)<br />autorstwa Yuval Noah Harari | <small title="Średnia ocen">8.3⭐</small> <small>9123 ocen</small>
- [Sztuka obsługi penisa](https://lubimyczytac.pl/ksiazka/4818570/sztuka-obslugi-penisa)<br />autorstwa Przemysław Pilarski,Andrzej Gryżewski | <small title="Średnia ocen">8.5⭐</small> <small>3333 ocen</small>
- [Krótka historia prawie wszystkiego](https://lubimyczytac.pl/ksiazka/3749344/krotka-historia-prawie-wszystkiego)<br />autorstwa Bill Bryson | <small title="Średnia ocen">8.2⭐</small> <small>2123 ocen</small>

## Ostateczny skrypt

Ostateczna wersja naszego skryptu wygląda tak:

```python
from bs4 import BeautifulSoup
from operator import itemgetter
from requests import get

def scrape_books(start_page=1, stop_page=11, average_rating_threshold=8.0, ratings_threshold=2000):
    if not (isinstance(start_page, int) and isinstance(stop_page, int) and isinstance(average_rating_threshold, float) and isinstance(ratings_threshold, int)):
        raise TypeError("Incompatible types of arguments")

    if not (start_page > 0 and stop_page > 0 and stop_page > start_page and 10.0 >= average_rating_threshold >= 0.0 and ratings_threshold > 0):
        raise TypeError("Incompatible values of arguments")

    try:
        base_url = "https://lubimyczytac.pl"
        best_books = []
        for page in range(start_page, stop_page):
            url = f"{base_url}/katalog?page={page}&listId=booksFilteredList&category[]=68&rating[]=0&rating[]=10&publishedYear[]=1200&publishedYear[]=2022&catalogSortBy=ratings-desc&paginatorType=Standard"
            res = get(url)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            books = soup.select(".authorAllBooks__single")

            for book in books:
                average_rating = float(book.select_one(
                    ".listLibrary__ratingStarsNumber").get_text(strip=True).replace(",", "."))
                ratings_string = book.select_one(
                    ".listLibrary__ratingAll").get_text(strip=True).split()[0]
                ratings = int(0 if ratings_string ==
                              "ocen" else ratings_string)

                if average_rating > average_rating_threshold and ratings > ratings_threshold:
                    title = book.select_one(
                        ".authorAllBooks__singleTextTitle").get_text(strip=True)
                    author = book.select_one(
                        ".authorAllBooks__singleTextAuthor").get_text(strip=True)
                    link = book.select_one(
                        ".authorAllBooks__singleTextTitle")["href"]
                    best_book = {"author": author, "title": title,
                                 "average_rating": average_rating, "ratings": ratings, "link": f"{base_url}{link}"}
                    best_books.append(best_book)

        return best_books

    except Exception as err:
        print(f"There was a problem during scraping books: {err}")

def save_books(book_list=[], subject="subject"):
    if not (isinstance(book_list, list) and isinstance(subject, str)):
        raise TypeError("Incompatible types of arguments")
    if len(book_list) > 0:
        file = open(f"najlepsze-ksiazki-{subject}.md", "w")
        try:
            file.write(f"## Najlepsze książki z kategorii: {subject}\n")
            for book in book_list:
                title, author, average_rating, ratings, link = itemgetter(
                    "title", "author", "average_rating", "ratings", "link")(book)
                list_item = f'- [{title}]({link})<br />autorstwa {author} | <small title="Średnia ocen">{average_rating}⭐</small> <small>{ratings} ocen</small>\n'
                file.write(list_item)
        finally:
            file.close()

best_books = scrape_books()
save_books(best_books, "popularnonaukowe")
```

Zrobiłem małą refaktoryzację. Umieściłem kod odpowiedzialny za scraping i zapisywanie w oddzielnych funkcjach. Mają domyślne parametry, dlatego nie musisz podawać ich wszystkich. Możesz wywołać te funkcje wiele razy dla różnych kategorii. Dodałem też podstawową obsługę błędów.
